{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRysG8qN2Rolnw0JpvDHot",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Blackbadger313/fingerprintPattern-CNN/blob/main/Fingerprint_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y8JEwAymBV7"
      },
      "outputs": [],
      "source": [
        "!!pip install fingerprint-enhancer\n",
        "!!pip install fingerprint-feature-extractor\n",
        "!!pip install Pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import load_model\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "wDiWLsvqmEly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fetch data set then do extraction\n",
        "!wget -N https://cainvas-static.s3.amazonaws.com/media/user_data/cainvas-admin/dataset_HFu5SVU.zip\n",
        "!unzip -qo dataset_HFu5SVU.zip\n",
        "dir = 'dataset'"
      ],
      "metadata": {
        "id": "hjkCSNgamHws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_picture_files(directory):\n",
        "    picture_extensions = ['.png']\n",
        "    file_names = []\n",
        "    for file in os.listdir(directory):\n",
        "        if os.path.isfile(os.path.join(directory, file)):\n",
        "            file_extension = os.path.splitext(file)[1].lower()\n",
        "            if file_extension in picture_extensions:\n",
        "                file_name = file  # Get file name without extension\n",
        "                file_names.append(file_name)\n",
        "    return file_names\n",
        "\n",
        "def list_txt_files(directory):\n",
        "    picture_extensions = ['.txt']\n",
        "    file_names = []\n",
        "    for file in os.listdir(directory):\n",
        "        if os.path.isfile(os.path.join(directory, file)):\n",
        "            file_extension = os.path.splitext(file)[1].lower()\n",
        "            if file_extension in picture_extensions:\n",
        "                file_name = file  # Get file name without extension\n",
        "                file_names.append(file_name)\n",
        "    return file_names\n",
        "\n",
        "# Replace 'directory_path' with the path to the directory you want to list the files from\n",
        "dataset_folder = '/content/dataset/'\n",
        "file_list = list_picture_files(dataset_folder)\n",
        "txt_list = list_txt_files(dataset_folder)"
      ],
      "metadata": {
        "id": "K7B7OpTQmK5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the library\n",
        "import fingerprint_enhancer\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "output_folder = '/content/drive/MyDrive/EnhancedFingerprintImage/'  # Output folder path in Google Drive\n",
        "\n",
        "# Create the output folder if it doesn't exist\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "with tqdm(total=len(txt_list), desc=\"Copying txt files\", unit=\"file\") as pbar:\n",
        "    for file_name in txt_list:\n",
        "        source_path = os.path.join(dataset_folder, file_name)\n",
        "        destination_path = os.path.join(output_folder, file_name)\n",
        "        shutil.copy2(source_path, destination_path)\n",
        "        pbar.update(1)\n",
        "\n",
        "with tqdm(total=len(file_list), desc=\"Processing picture files\", unit=\"file\") as pbar:\n",
        "    for file_name in file_list:\n",
        "        img = cv2.imread(os.path.join(dataset_folder, file_name), 0)  # Read input image\n",
        "        out = fingerprint_enhancer.enhance_Fingerprint(img, resize=True)  # Enhance the fingerprint image\n",
        "        output_path = os.path.join(output_folder, file_name)\n",
        "        cv2.imwrite(output_path, out)\n",
        "        origina_size = Image.open(output_path)\n",
        "        resized_image = origina_size.resize((128, 128))\n",
        "        resized_image.save(output_path)\n",
        "        pbar.update(1)\n",
        "\n",
        "print('Processing complete!')\n"
      ],
      "metadata": {
        "id": "iXHwDOXBmNHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "output_folder = '/content/drive/MyDrive/EnhancedFingerprintImage/'"
      ],
      "metadata": {
        "id": "ti40TbiLmPMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "img_names = []\n",
        "img_paths = []\n",
        "gender = []\n",
        "files = os.listdir(output_folder)\n",
        "\n",
        "with tqdm(total=len(files), desc='Processing files') as pbar:\n",
        "    for file in files:\n",
        "        if file.endswith('.txt'):\n",
        "            with open(os.path.join(output_folder, file), 'r') as t:\n",
        "                content = t.readlines()\n",
        "                gender.append(content[0].rsplit(' ')[1][0])\n",
        "                img_name = content[2].rsplit(' ')[1][:-4] + '.png'\n",
        "                img_paths.append(os.path.join(output_folder, img_name))\n",
        "                img_names.append(img_name)\n",
        "                labels.append(content[1].rsplit(' ')[1][0])\n",
        "\n",
        "        pbar.update(1)"
      ],
      "metadata": {
        "id": "VIacQLdHmRzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fill up data to data frame\n",
        "print(labels)\n",
        "print(img_names)\n",
        "print(img_paths)\n",
        "print(gender)\n",
        "df = pd.DataFrame()\n",
        "df['IMAGE PATH'] = img_paths\n",
        "df['IMAGE NAME'] = img_names\n",
        "df['LABEL'] = labels\n",
        "df['GENDER'] = gender"
      ],
      "metadata": {
        "id": "Ovq0ygbtmTlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if data already rebalance one to each others (optional)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "sns.countplot(ax=axes[0], data = df, x = 'LABEL')\n",
        "sns.countplot(ax=axes[1], data = df, x = 'LABEL', hue = 'GENDER')"
      ],
      "metadata": {
        "id": "RYpjRjowmVQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns = 'GENDER',inplace=True) #drop table gender\n",
        "df.head()"
      ],
      "metadata": {
        "id": "9o4be68hmXE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rename (optional)\n",
        "df['MAPPED LABELS'] = [map_classes[i] for i in df['LABEL']]\n",
        "df = df.sample(frac = 1)\n",
        "df.to_csv('dataset.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "eNufuk13mYwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show data (optional)\n",
        "dim = len(classes)\n",
        "fig,axes = plt.subplots(1,dim)\n",
        "fig.subplots_adjust(0,0,2,2)\n",
        "for idx, i in enumerate(classes):\n",
        "    dum = df[df['LABEL'] == i]\n",
        "    random_num = random.choice(dum.index)\n",
        "    label = df.loc[random_num]['LABEL']\n",
        "    axes[idx].imshow(cv2.imread(df.loc[random_num]['IMAGE PATH']))\n",
        "    axes[idx].set_title(\"CLASS: \"+label +\"\\n\" +  \"LABEL:\"+str(map_classes[label]))\n",
        "    axes[idx].axis('off')"
      ],
      "metadata": {
        "id": "T4aWx38xoUQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fingerprint_feature_extractor\n",
        "\n",
        "X_data = df['IMAGE PATH']\n",
        "y_data = df['MAPPED LABELS']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, shuffle=True, test_size=0.2,stratify=y_data)\n",
        "#Creating numpy arrays of images\n",
        "X = []\n",
        "y = []\n",
        "for img in X_train:\n",
        "    #FeaturesTerminations, FeaturesBifurcations = fingerprint_feature_extractor.extract_minutiae_features(img, spuriousMinutiaeThresh=10, invertImage=False, showResult=True, saveResult=True)\n",
        "    #feature_concatenate = np.concatenate((FeaturesTerminations, FeaturesBifurcations))\n",
        "    #print(feature_concatenate.flatten())\n",
        "    #X.append(feature_concatenate.flatten())\n",
        "    X.append(cv2.imread(img))\n",
        "    print(cv2.imread(img))\n",
        "for i in y_train:\n",
        "    y.append(i)\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Converting the labels vector to one-hot format\n",
        "y = keras.utils.to_categorical(y, 5)"
      ],
      "metadata": {
        "id": "ZVz1cMw8oWnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total number of Images: {len(X_data)}\")\n",
        "print(f\"Number of Training Images: {len(X_train)}\")\n",
        "print(f\"Number of Test Images: {len(X_test)}\") # Saving a small number of images for model testing|\n",
        "print(f\"Shape of Images: {X[0].shape}\") #Printing the shape of Images"
      ],
      "metadata": {
        "id": "G051kXsVoYef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.Conv2D(32, input_shape=(128,128,3),padding=\"same\",kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), padding=\"same\",activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), padding=\"same\",activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3),padding=\"same\",activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3),padding=\"same\",activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(128, kernel_size=(3, 3),padding=\"same\",activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(5, activation=\"softmax\",kernel_regularizer='l1_l2'),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "yc7lLlx9obaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#configure hyperparameters\n",
        "LOSS = 'categorical_crossentropy'\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 0.002\n",
        "OPTIMIZER = keras.optimizers.Adam(learning_rate=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "aaMr5vo6odmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=['accuracy'])\n",
        "\n",
        "history=model.fit(x=X, y=y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.2)"
      ],
      "metadata": {
        "id": "qK9CjA_6of7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving the model\n",
        "model.save('enhancedfingerprint.h5')"
      ],
      "metadata": {
        "id": "3mSu0UiQoixv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Optional\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'val'], loc='center right')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j9Y0WqBaokma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert testing data\n",
        "test_X = []\n",
        "for i in X_test:\n",
        "    im = cv2.imread(i)\n",
        "    print(im)\n",
        "    im = np.reshape(im, (1,128,128,3))\n",
        "    test_X.append(im)\n",
        "test_X = np.array(test_X)"
      ],
      "metadata": {
        "id": "rXMcGLouomHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(5, 5)\n",
        "fig.subplots_adjust(0, 0, 3, 3)\n",
        "\n",
        "f1_scores = []\n",
        "precision_scores = []\n",
        "accuracy_scores = []\n",
        "recall_scores = []\n",
        "\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        num = random.randint(0, len(test_X) - 1)\n",
        "        display_image = test_X[num].squeeze(0)\n",
        "        image = test_X[num]\n",
        "        predicted_prob = model.predict(image)\n",
        "        predicted_class = np.argmax(predicted_prob)\n",
        "        ground_truth = classes[y_test.iloc[num]]\n",
        "        axes[i, j].imshow(display_image)\n",
        "        if classes[predicted_class] != classes[y_test.iloc[num]]:\n",
        "            t = 'PREDICTED {} \\n GROUND TRUTH[{}]'.format(classes[predicted_class], classes[y_test.iloc[num]])\n",
        "            axes[i, j].set_title(t, fontdict={'color': 'darkred'})\n",
        "        else:\n",
        "            t = '[CORRECT] {}'.format(classes[predicted_class])\n",
        "            axes[i, j].set_title(t)\n",
        "        axes[i, j].axis('off')\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        predicted_labels = np.array([classes[predicted_class]])\n",
        "        ground_truth_labels = np.array([ground_truth])\n",
        "        f1_scores.append(f1_score(ground_truth_labels, predicted_labels, average='macro'))\n",
        "        precision_scores.append(precision_score(ground_truth_labels, predicted_labels, average='macro'))\n",
        "        accuracy_scores.append(accuracy_score(ground_truth_labels, predicted_labels))\n",
        "        recall_scores.append(recall_score(ground_truth_labels, predicted_labels, average='macro'))\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Print average evaluation scores\n",
        "print(\"Average F1 Score:\", np.mean(f1_scores))\n",
        "print(\"Average Precision:\", np.mean(precision_scores))\n",
        "print(\"Average Accuracy:\", np.mean(accuracy_scores))\n",
        "print(\"Average Recall:\", np.mean(recall_scores))\n"
      ],
      "metadata": {
        "id": "C1DWzBQZooI3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}